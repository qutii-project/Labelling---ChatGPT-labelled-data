{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7581373,"sourceType":"datasetVersion","datasetId":4413248},{"sourceId":7572277,"sourceType":"datasetVersion","datasetId":4408277}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This model is to predict labels for different question and answers. The question and anwers are categorised to various categories like science and tech, analysis ,taxonomy, factual, management, strategy and  ethics and regulation. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.043214Z","iopub.execute_input":"2024-02-07T14:52:49.043738Z","iopub.status.idle":"2024-02-07T14:52:49.050104Z","shell.execute_reply.started":"2024-02-07T14:52:49.043695Z","shell.execute_reply":"2024-02-07T14:52:49.048741Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"dataset1 = pd.read_csv('/kaggle/input/new-data-smart-urban-mobility/labelled_Smart_City_data_01_No_Undesirable.csv', encoding='ISO-8859-1',on_bad_lines='skip')\ndataset1","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.070956Z","iopub.execute_input":"2024-02-07T14:52:49.072136Z","iopub.status.idle":"2024-02-07T14:52:49.121993Z","shell.execute_reply.started":"2024-02-07T14:52:49.072087Z","shell.execute_reply":"2024-02-07T14:52:49.121110Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"                                              Question  \\\n0    What is the main strategy of Smart Cities like...   \n1    What are the main components of Barcelona's Sm...   \n2    Why are governments investing in advanced infr...   \n3    How are cities transforming to enhance their i...   \n4    How has Barcelona transformed itself into a Sm...   \n..                                                 ...   \n589  How did South Korea control COVID-19 panic buy...   \n590  What measures did South Korea take to track an...   \n591  How did South Korea flatten its coronavirus cu...   \n592  What was the secret to South Korea's success i...   \n593  What did the World Health Organization state r...   \n\n                                                Answer       Label  \\\n0    Smart Cities like Barcelona base their strateg...    Strategy   \n1    The main components of Barcelona's Smart City ...    Strategy   \n2    Governments are investing in advanced infrastr...  Management   \n3    Cities are undergoing fundamental transformati...    Strategy   \n4    Barcelona has undertaken significant reforms t...  Management   \n..                                                 ...         ...   \n589  South Korea was able to control COVID-19 panic...  Management   \n590  South Korea implemented smartphone apps to tra...  Management   \n591  South Korea was able to flatten its coronaviru...    Strategy   \n592  The secret to South Korea's success in handlin...    Analysis   \n593  The World Health Organization stated that they...     Factual   \n\n                                            Source URL  \\\n0    https://link.springer.com/article/10.1007/s131...   \n1    https://link.springer.com/article/10.1007/s131...   \n2    https://link.springer.com/article/10.1007/s131...   \n3    https://link.springer.com/article/10.1007/s131...   \n4    https://link.springer.com/article/10.1007/s131...   \n..                                                 ...   \n589  https://penanginstitute.org/wp-content/uploads...   \n590  https://penanginstitute.org/wp-content/uploads...   \n591  https://penanginstitute.org/wp-content/uploads...   \n592  https://penanginstitute.org/wp-content/uploads...   \n593  https://penanginstitute.org/wp-content/uploads...   \n\n                                      Source Title  \\\n0    A Smart City Initiative the Case of Barcelona   \n1    A Smart City Initiative the Case of Barcelona   \n2    A Smart City Initiative the Case of Barcelona   \n3    A Smart City Initiative the Case of Barcelona   \n4    A Smart City Initiative the Case of Barcelona   \n..                                             ...   \n589       Smart City Technologies Take on COVID-57   \n590       Smart City Technologies Take on COVID-58   \n591       Smart City Technologies Take on COVID-59   \n592       Smart City Technologies Take on COVID-60   \n593       Smart City Technologies Take on COVID-61   \n\n                            Source Author  \n0    Basic, T., Admiral, E. & Wareham, J.  \n1    Basic, T., Admiral, E. & Wareham, J.  \n2    Basic, T., Admiral, E. & Wareham, J.  \n3    Basic, T., Admiral, E. & Wareham, J.  \n4    Basic, T., Admiral, E. & Wareham, J.  \n..                                    ...  \n589                           Tan Lie Inn  \n590                           Tan Lie Inn  \n591                           Tan Lie Inn  \n592                           Tan Lie Inn  \n593                           Tan Lie Inn  \n\n[594 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Label</th>\n      <th>Source URL</th>\n      <th>Source Title</th>\n      <th>Source Author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the main strategy of Smart Cities like...</td>\n      <td>Smart Cities like Barcelona base their strateg...</td>\n      <td>Strategy</td>\n      <td>https://link.springer.com/article/10.1007/s131...</td>\n      <td>A Smart City Initiative the Case of Barcelona</td>\n      <td>Basic, T., Admiral, E. &amp; Wareham, J.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are the main components of Barcelona's Sm...</td>\n      <td>The main components of Barcelona's Smart City ...</td>\n      <td>Strategy</td>\n      <td>https://link.springer.com/article/10.1007/s131...</td>\n      <td>A Smart City Initiative the Case of Barcelona</td>\n      <td>Basic, T., Admiral, E. &amp; Wareham, J.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why are governments investing in advanced infr...</td>\n      <td>Governments are investing in advanced infrastr...</td>\n      <td>Management</td>\n      <td>https://link.springer.com/article/10.1007/s131...</td>\n      <td>A Smart City Initiative the Case of Barcelona</td>\n      <td>Basic, T., Admiral, E. &amp; Wareham, J.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How are cities transforming to enhance their i...</td>\n      <td>Cities are undergoing fundamental transformati...</td>\n      <td>Strategy</td>\n      <td>https://link.springer.com/article/10.1007/s131...</td>\n      <td>A Smart City Initiative the Case of Barcelona</td>\n      <td>Basic, T., Admiral, E. &amp; Wareham, J.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How has Barcelona transformed itself into a Sm...</td>\n      <td>Barcelona has undertaken significant reforms t...</td>\n      <td>Management</td>\n      <td>https://link.springer.com/article/10.1007/s131...</td>\n      <td>A Smart City Initiative the Case of Barcelona</td>\n      <td>Basic, T., Admiral, E. &amp; Wareham, J.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>589</th>\n      <td>How did South Korea control COVID-19 panic buy...</td>\n      <td>South Korea was able to control COVID-19 panic...</td>\n      <td>Management</td>\n      <td>https://penanginstitute.org/wp-content/uploads...</td>\n      <td>Smart City Technologies Take on COVID-57</td>\n      <td>Tan Lie Inn</td>\n    </tr>\n    <tr>\n      <th>590</th>\n      <td>What measures did South Korea take to track an...</td>\n      <td>South Korea implemented smartphone apps to tra...</td>\n      <td>Management</td>\n      <td>https://penanginstitute.org/wp-content/uploads...</td>\n      <td>Smart City Technologies Take on COVID-58</td>\n      <td>Tan Lie Inn</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>How did South Korea flatten its coronavirus cu...</td>\n      <td>South Korea was able to flatten its coronaviru...</td>\n      <td>Strategy</td>\n      <td>https://penanginstitute.org/wp-content/uploads...</td>\n      <td>Smart City Technologies Take on COVID-59</td>\n      <td>Tan Lie Inn</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>What was the secret to South Korea's success i...</td>\n      <td>The secret to South Korea's success in handlin...</td>\n      <td>Analysis</td>\n      <td>https://penanginstitute.org/wp-content/uploads...</td>\n      <td>Smart City Technologies Take on COVID-60</td>\n      <td>Tan Lie Inn</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>What did the World Health Organization state r...</td>\n      <td>The World Health Organization stated that they...</td>\n      <td>Factual</td>\n      <td>https://penanginstitute.org/wp-content/uploads...</td>\n      <td>Smart City Technologies Take on COVID-61</td>\n      <td>Tan Lie Inn</td>\n    </tr>\n  </tbody>\n</table>\n<p>594 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_1a=dataset1[[\"Question\",\"Answer\",\"Label\"]]\n","metadata":{"id":"O5M6FoO6iInX","outputId":"090c6eb8-c134-4268-fb64-1ea012860be4","execution":{"iopub.status.busy":"2024-02-07T14:52:49.123597Z","iopub.execute_input":"2024-02-07T14:52:49.124381Z","iopub.status.idle":"2024-02-07T14:52:49.130550Z","shell.execute_reply.started":"2024-02-07T14:52:49.124345Z","shell.execute_reply":"2024-02-07T14:52:49.129455Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"dataset_1a = dataset_1a.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.132175Z","iopub.execute_input":"2024-02-07T14:52:49.132842Z","iopub.status.idle":"2024-02-07T14:52:49.145836Z","shell.execute_reply.started":"2024-02-07T14:52:49.132804Z","shell.execute_reply":"2024-02-07T14:52:49.144724Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"dataset_1a","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.148902Z","iopub.execute_input":"2024-02-07T14:52:49.149333Z","iopub.status.idle":"2024-02-07T14:52:49.165978Z","shell.execute_reply.started":"2024-02-07T14:52:49.149246Z","shell.execute_reply":"2024-02-07T14:52:49.164930Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"                                              Question  \\\n0    What is the main strategy of Smart Cities like...   \n1    What are the main components of Barcelona's Sm...   \n2    Why are governments investing in advanced infr...   \n3    How are cities transforming to enhance their i...   \n4    How has Barcelona transformed itself into a Sm...   \n..                                                 ...   \n589  How did South Korea control COVID-19 panic buy...   \n590  What measures did South Korea take to track an...   \n591  How did South Korea flatten its coronavirus cu...   \n592  What was the secret to South Korea's success i...   \n593  What did the World Health Organization state r...   \n\n                                                Answer       Label  \n0    Smart Cities like Barcelona base their strateg...    Strategy  \n1    The main components of Barcelona's Smart City ...    Strategy  \n2    Governments are investing in advanced infrastr...  Management  \n3    Cities are undergoing fundamental transformati...    Strategy  \n4    Barcelona has undertaken significant reforms t...  Management  \n..                                                 ...         ...  \n589  South Korea was able to control COVID-19 panic...  Management  \n590  South Korea implemented smartphone apps to tra...  Management  \n591  South Korea was able to flatten its coronaviru...    Strategy  \n592  The secret to South Korea's success in handlin...    Analysis  \n593  The World Health Organization stated that they...     Factual  \n\n[594 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the main strategy of Smart Cities like...</td>\n      <td>Smart Cities like Barcelona base their strateg...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are the main components of Barcelona's Sm...</td>\n      <td>The main components of Barcelona's Smart City ...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why are governments investing in advanced infr...</td>\n      <td>Governments are investing in advanced infrastr...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How are cities transforming to enhance their i...</td>\n      <td>Cities are undergoing fundamental transformati...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How has Barcelona transformed itself into a Sm...</td>\n      <td>Barcelona has undertaken significant reforms t...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>589</th>\n      <td>How did South Korea control COVID-19 panic buy...</td>\n      <td>South Korea was able to control COVID-19 panic...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>590</th>\n      <td>What measures did South Korea take to track an...</td>\n      <td>South Korea implemented smartphone apps to tra...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>591</th>\n      <td>How did South Korea flatten its coronavirus cu...</td>\n      <td>South Korea was able to flatten its coronaviru...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>What was the secret to South Korea's success i...</td>\n      <td>The secret to South Korea's success in handlin...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>What did the World Health Organization state r...</td>\n      <td>The World Health Organization stated that they...</td>\n      <td>Factual</td>\n    </tr>\n  </tbody>\n</table>\n<p>594 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_1b = pd.read_csv('/kaggle/input/new-data-smart-urban-mobility/labelled_Smart_City_data_02_No_Undesirable.csv', encoding='ISO-8859-1',on_bad_lines='skip')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.167801Z","iopub.execute_input":"2024-02-07T14:52:49.168704Z","iopub.status.idle":"2024-02-07T14:52:49.214585Z","shell.execute_reply.started":"2024-02-07T14:52:49.168633Z","shell.execute_reply":"2024-02-07T14:52:49.213364Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"dataset_1b=dataset_1b[[\"Question\",\"Answer\",\"Label\"]]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.217742Z","iopub.execute_input":"2024-02-07T14:52:49.218253Z","iopub.status.idle":"2024-02-07T14:52:49.225899Z","shell.execute_reply.started":"2024-02-07T14:52:49.218203Z","shell.execute_reply":"2024-02-07T14:52:49.224696Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"dataset_1b = dataset_1b.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.228590Z","iopub.execute_input":"2024-02-07T14:52:49.228996Z","iopub.status.idle":"2024-02-07T14:52:49.240971Z","shell.execute_reply.started":"2024-02-07T14:52:49.228958Z","shell.execute_reply":"2024-02-07T14:52:49.239932Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"dataset_1b","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.242452Z","iopub.execute_input":"2024-02-07T14:52:49.243112Z","iopub.status.idle":"2024-02-07T14:52:49.259493Z","shell.execute_reply.started":"2024-02-07T14:52:49.243062Z","shell.execute_reply":"2024-02-07T14:52:49.258400Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"                                               Question  \\\n0     What is the ranking of Bandung City Government...   \n1     What are the three core activities in the smar...   \n2     What is the purpose of the smart environment p...   \n3     What are some initiatives that have been imple...   \n4     What is the impact of increased population in ...   \n...                                                 ...   \n2669  How can open innovation contribute to the deve...   \n2670  According to Singh (1997), what factors impact...   \n2671  What is the estimated market opportunity for s...   \n2672   What is the market opportunity for smart cities?   \n2673  Where can I find more information about the fu...   \n\n                                                 Answer                  Label  \n0     Bandung City Government is ranked the 3rd in t...               Analysis  \n1     The three core activities in the smart environ...             Management  \n2     The purpose of the smart environment program i...             Management  \n3     Some initiatives that have been implemented as...             Management  \n4     The increased population in Bandung puts a hig...               Analysis  \n...                                                 ...                    ...  \n2669  Open innovation encourages collaboration and s...             Management  \n2670  Technological complexity and interfirm coopera...       Science and Tech  \n2671                                      1.5 trillion.               Analysis  \n2672  The market opportunity for smart cities is est...               Analysis  \n2673  You can find more information about the future...  Ethics and Regulation  \n\n[2674 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the ranking of Bandung City Government...</td>\n      <td>Bandung City Government is ranked the 3rd in t...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are the three core activities in the smar...</td>\n      <td>The three core activities in the smart environ...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is the purpose of the smart environment p...</td>\n      <td>The purpose of the smart environment program i...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What are some initiatives that have been imple...</td>\n      <td>Some initiatives that have been implemented as...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the impact of increased population in ...</td>\n      <td>The increased population in Bandung puts a hig...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>How can open innovation contribute to the deve...</td>\n      <td>Open innovation encourages collaboration and s...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>According to Singh (1997), what factors impact...</td>\n      <td>Technological complexity and interfirm coopera...</td>\n      <td>Science and Tech</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>What is the estimated market opportunity for s...</td>\n      <td>1.5 trillion.</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>2672</th>\n      <td>What is the market opportunity for smart cities?</td>\n      <td>The market opportunity for smart cities is est...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>2673</th>\n      <td>Where can I find more information about the fu...</td>\n      <td>You can find more information about the future...</td>\n      <td>Ethics and Regulation</td>\n    </tr>\n  </tbody>\n</table>\n<p>2674 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_1c = pd.read_csv('/kaggle/input/new-data-smart-urban-mobility/labelled_Sustainable_Urban_Mobility_data_01_No_Undesirable.csv', encoding='ISO-8859-1',on_bad_lines='skip')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.260996Z","iopub.execute_input":"2024-02-07T14:52:49.261759Z","iopub.status.idle":"2024-02-07T14:52:49.287742Z","shell.execute_reply.started":"2024-02-07T14:52:49.261721Z","shell.execute_reply":"2024-02-07T14:52:49.286755Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"dataset_1c=dataset_1c[[\"Question\",\"Answer\",\"Label\"]]","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.289561Z","iopub.execute_input":"2024-02-07T14:52:49.290383Z","iopub.status.idle":"2024-02-07T14:52:49.298025Z","shell.execute_reply.started":"2024-02-07T14:52:49.290335Z","shell.execute_reply":"2024-02-07T14:52:49.296778Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"dataset_1c = dataset_1c.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.299573Z","iopub.execute_input":"2024-02-07T14:52:49.300819Z","iopub.status.idle":"2024-02-07T14:52:49.316226Z","shell.execute_reply.started":"2024-02-07T14:52:49.300770Z","shell.execute_reply":"2024-02-07T14:52:49.314969Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"dataset_1c","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.318250Z","iopub.execute_input":"2024-02-07T14:52:49.318629Z","iopub.status.idle":"2024-02-07T14:52:49.342190Z","shell.execute_reply.started":"2024-02-07T14:52:49.318594Z","shell.execute_reply":"2024-02-07T14:52:49.340761Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"                                               Question  \\\n0     What is the purpose of the integrated cooling ...   \n1      How was the integrated cooling system validated?   \n2     How does the proposed system under heat pump s...   \n3     What characteristics of the proposed system co...   \n4     How can the proposed system be integrated into...   \n...                                                 ...   \n1353                What is the topic of the paragraph?   \n1354  Where was the 2019 IEEE 17th International Con...   \n1355  What is the purpose of the Real-Time Traffic P...   \n1356  What is the focus of the Smart Traffic Congest...   \n1357  What is the purpose of using blockchain in log...   \n\n                                                 Answer             Label  \n0     The purpose of the integrated cooling system i...          Analysis  \n1     The integrated cooling system was validated th...  Science and Tech  \n2     The proposed system under heat pump supplement...          Strategy  \n3     The heat and electric co-generation characteri...          Analysis  \n4     The proposed system can provide domestic cooli...        Management  \n...                                                 ...               ...  \n1353  The topic of the paragraph is about smart cont...        Management  \n1354  The 2019 IEEE 17th International Conference on...           Factual  \n1355  The purpose is to collect and analyse traffic ...          Analysis  \n1356  The focus is to review the implementation of s...          Analysis  \n1357  The purpose of using blockchain in logistics i...        Management  \n\n[1358 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the purpose of the integrated cooling ...</td>\n      <td>The purpose of the integrated cooling system i...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How was the integrated cooling system validated?</td>\n      <td>The integrated cooling system was validated th...</td>\n      <td>Science and Tech</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How does the proposed system under heat pump s...</td>\n      <td>The proposed system under heat pump supplement...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What characteristics of the proposed system co...</td>\n      <td>The heat and electric co-generation characteri...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How can the proposed system be integrated into...</td>\n      <td>The proposed system can provide domestic cooli...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1353</th>\n      <td>What is the topic of the paragraph?</td>\n      <td>The topic of the paragraph is about smart cont...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>1354</th>\n      <td>Where was the 2019 IEEE 17th International Con...</td>\n      <td>The 2019 IEEE 17th International Conference on...</td>\n      <td>Factual</td>\n    </tr>\n    <tr>\n      <th>1355</th>\n      <td>What is the purpose of the Real-Time Traffic P...</td>\n      <td>The purpose is to collect and analyse traffic ...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1356</th>\n      <td>What is the focus of the Smart Traffic Congest...</td>\n      <td>The focus is to review the implementation of s...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1357</th>\n      <td>What is the purpose of using blockchain in log...</td>\n      <td>The purpose of using blockchain in logistics i...</td>\n      <td>Management</td>\n    </tr>\n  </tbody>\n</table>\n<p>1358 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_1d = pd.read_csv('/kaggle/input/new-data-smart-urban-mobility/labelled_Sustainable_Urban_Mobility_data_02_No_Undesirable.csv', encoding='ISO-8859-1',on_bad_lines='skip')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.345293Z","iopub.execute_input":"2024-02-07T14:52:49.346017Z","iopub.status.idle":"2024-02-07T14:52:49.366955Z","shell.execute_reply.started":"2024-02-07T14:52:49.345977Z","shell.execute_reply":"2024-02-07T14:52:49.365419Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"dataset_1d=dataset_1d[[\"Question\",\"Answer\",\"Label\"]]","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.368920Z","iopub.execute_input":"2024-02-07T14:52:49.369315Z","iopub.status.idle":"2024-02-07T14:52:49.376694Z","shell.execute_reply.started":"2024-02-07T14:52:49.369281Z","shell.execute_reply":"2024-02-07T14:52:49.375464Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"dataset_1d = dataset_1d.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.378298Z","iopub.execute_input":"2024-02-07T14:52:49.379573Z","iopub.status.idle":"2024-02-07T14:52:49.391059Z","shell.execute_reply.started":"2024-02-07T14:52:49.379528Z","shell.execute_reply":"2024-02-07T14:52:49.390115Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"dataset_1d","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.392403Z","iopub.execute_input":"2024-02-07T14:52:49.392861Z","iopub.status.idle":"2024-02-07T14:52:49.411879Z","shell.execute_reply.started":"2024-02-07T14:52:49.392819Z","shell.execute_reply":"2024-02-07T14:52:49.410713Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"                                               Question  \\\n0     What was the European Commission's previous ap...   \n1     What were the main reasons for the European Co...   \n2     What were the recommendations made in the Comm...   \n3     What was the first Action identified in the Eu...   \n4     What are the broad headings under which the sp...   \n...                                                 ...   \n1223  What is the focus of the book \"Intelligent Cit...   \n1224  What are some key topics covered in the paragr...   \n1225      What is the Warsaw Development Strategy 2020?   \n1226  What is the Sustainable Transport Plan for War...   \n1227  How does urban structure affect individual tra...   \n\n                                                 Answer                  Label  \n0     The European Commission's previous approach to...  Ethics and Regulation  \n1     The European Commission's analysis showed that...               Analysis  \n2     The Action Plan recommended encouraging the ad...               Strategy  \n3     The first Action identified in the European Co...             Management  \n4     The specific activities for preparing Sustaina...             Management  \n...                                                 ...                    ...  \n1223  The focus of the book is on the tools and tech...               Strategy  \n1224  The key topics covered in this paragraph inclu...               Analysis  \n1225  The Warsaw Development Strategy 2020 is a plan...               Strategy  \n1226  The Sustainable Transport Plan for Warsaw is a...               Strategy  \n1227  Urban structure has a significant influence on...               Analysis  \n\n[1228 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What was the European Commission's previous ap...</td>\n      <td>The European Commission's previous approach to...</td>\n      <td>Ethics and Regulation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What were the main reasons for the European Co...</td>\n      <td>The European Commission's analysis showed that...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What were the recommendations made in the Comm...</td>\n      <td>The Action Plan recommended encouraging the ad...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What was the first Action identified in the Eu...</td>\n      <td>The first Action identified in the European Co...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What are the broad headings under which the sp...</td>\n      <td>The specific activities for preparing Sustaina...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1223</th>\n      <td>What is the focus of the book \"Intelligent Cit...</td>\n      <td>The focus of the book is on the tools and tech...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>1224</th>\n      <td>What are some key topics covered in the paragr...</td>\n      <td>The key topics covered in this paragraph inclu...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1225</th>\n      <td>What is the Warsaw Development Strategy 2020?</td>\n      <td>The Warsaw Development Strategy 2020 is a plan...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>1226</th>\n      <td>What is the Sustainable Transport Plan for War...</td>\n      <td>The Sustainable Transport Plan for Warsaw is a...</td>\n      <td>Strategy</td>\n    </tr>\n    <tr>\n      <th>1227</th>\n      <td>How does urban structure affect individual tra...</td>\n      <td>Urban structure has a significant influence on...</td>\n      <td>Analysis</td>\n    </tr>\n  </tbody>\n</table>\n<p>1228 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_1e = pd.read_csv('/kaggle/input/new-data-smart-urban-mobility/labelled_Sustainable_Urban_Mobility_data_03_No_Undesirable.csv', encoding='ISO-8859-1',on_bad_lines='skip')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.413399Z","iopub.execute_input":"2024-02-07T14:52:49.414439Z","iopub.status.idle":"2024-02-07T14:52:49.446000Z","shell.execute_reply.started":"2024-02-07T14:52:49.414396Z","shell.execute_reply":"2024-02-07T14:52:49.444912Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"dataset_1e=dataset_1e[[\"Question\",\"Answer\",\"Label\"]]","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.447196Z","iopub.execute_input":"2024-02-07T14:52:49.447529Z","iopub.status.idle":"2024-02-07T14:52:49.453418Z","shell.execute_reply.started":"2024-02-07T14:52:49.447500Z","shell.execute_reply":"2024-02-07T14:52:49.452439Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"dataset_1e = dataset_1e.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.454622Z","iopub.execute_input":"2024-02-07T14:52:49.455533Z","iopub.status.idle":"2024-02-07T14:52:49.468758Z","shell.execute_reply.started":"2024-02-07T14:52:49.455483Z","shell.execute_reply":"2024-02-07T14:52:49.467645Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"dataset_1e","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.473065Z","iopub.execute_input":"2024-02-07T14:52:49.474006Z","iopub.status.idle":"2024-02-07T14:52:49.490341Z","shell.execute_reply.started":"2024-02-07T14:52:49.473953Z","shell.execute_reply":"2024-02-07T14:52:49.488811Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"                                               Question  \\\n0     What factors contribute to increasing pollutan...   \n1     Which gases show a consistent relationship wit...   \n2     What is a serious problem in evaluating transp...   \n3     What factors influence commuters' satisfaction...   \n4     What is the percentage of residents travelling...   \n...                                                 ...   \n1743  Why is assessing current mobility conditions c...   \n1744  How are indicators used to assess sustainable ...   \n1745  What are some challenges in using indicator se...   \n1746  What are some potential drawbacks of autonomou...   \n1747  How does N2O emissions compare to CO2 emission...   \n\n                                                 Answer       Label  \n0     The amelioration of purchasing power, leading ...    Analysis  \n1     The gases CO2 and SPM show a consistent relati...    Analysis  \n2     A serious problem in evaluating transport and ...    Analysis  \n3     The activities that commuters engage in while ...    Analysis  \n4     The annual average concentration of NO2 and PM...    Analysis  \n...                                                 ...         ...  \n1743  Assessing current mobility conditions is neces...  Management  \n1744  Indicators are used to assess sustainable urba...  Management  \n1745  One of the challenges is the large number of i...  Management  \n1746  The large-scale potential of autonomous pooled...    Analysis  \n1747  N2O emissions contribute only a few percent to...    Analysis  \n\n[1748 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What factors contribute to increasing pollutan...</td>\n      <td>The amelioration of purchasing power, leading ...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which gases show a consistent relationship wit...</td>\n      <td>The gases CO2 and SPM show a consistent relati...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is a serious problem in evaluating transp...</td>\n      <td>A serious problem in evaluating transport and ...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What factors influence commuters' satisfaction...</td>\n      <td>The activities that commuters engage in while ...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the percentage of residents travelling...</td>\n      <td>The annual average concentration of NO2 and PM...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1743</th>\n      <td>Why is assessing current mobility conditions c...</td>\n      <td>Assessing current mobility conditions is neces...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>1744</th>\n      <td>How are indicators used to assess sustainable ...</td>\n      <td>Indicators are used to assess sustainable urba...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>1745</th>\n      <td>What are some challenges in using indicator se...</td>\n      <td>One of the challenges is the large number of i...</td>\n      <td>Management</td>\n    </tr>\n    <tr>\n      <th>1746</th>\n      <td>What are some potential drawbacks of autonomou...</td>\n      <td>The large-scale potential of autonomous pooled...</td>\n      <td>Analysis</td>\n    </tr>\n    <tr>\n      <th>1747</th>\n      <td>How does N2O emissions compare to CO2 emission...</td>\n      <td>N2O emissions contribute only a few percent to...</td>\n      <td>Analysis</td>\n    </tr>\n  </tbody>\n</table>\n<p>1748 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_1f = pd.read_csv('/kaggle/input/new-data-smart-urban-mobility/labelled_Contradictory_Sustainable_Urban_Mobility_data_No_Undesirable.csv', encoding='ISO-8859-1',on_bad_lines='skip')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.491915Z","iopub.execute_input":"2024-02-07T14:52:49.492292Z","iopub.status.idle":"2024-02-07T14:52:49.523446Z","shell.execute_reply.started":"2024-02-07T14:52:49.492258Z","shell.execute_reply":"2024-02-07T14:52:49.522442Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"dataset_1f=dataset_1f[[\"Question\",\"Answer\",\"Label\"]]","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.524852Z","iopub.execute_input":"2024-02-07T14:52:49.525398Z","iopub.status.idle":"2024-02-07T14:52:49.531512Z","shell.execute_reply.started":"2024-02-07T14:52:49.525365Z","shell.execute_reply":"2024-02-07T14:52:49.530301Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"dataset_1f = dataset_1f.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.533510Z","iopub.execute_input":"2024-02-07T14:52:49.534023Z","iopub.status.idle":"2024-02-07T14:52:49.547347Z","shell.execute_reply.started":"2024-02-07T14:52:49.533986Z","shell.execute_reply":"2024-02-07T14:52:49.546098Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"dataset_1 = pd.concat([dataset_1a, dataset_1b, dataset_1c, dataset_1d, dataset_1e, dataset_1f], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.548860Z","iopub.execute_input":"2024-02-07T14:52:49.549349Z","iopub.status.idle":"2024-02-07T14:52:49.562129Z","shell.execute_reply.started":"2024-02-07T14:52:49.549311Z","shell.execute_reply":"2024-02-07T14:52:49.561209Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"dataset_2 = dataset_1.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.563444Z","iopub.execute_input":"2024-02-07T14:52:49.563981Z","iopub.status.idle":"2024-02-07T14:52:49.583306Z","shell.execute_reply.started":"2024-02-07T14:52:49.563949Z","shell.execute_reply":"2024-02-07T14:52:49.582305Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"dataset_2['joined_column'] = dataset_2['Question'] + ' ' + dataset_2['Answer']\ndataset_3 =dataset_2[[\"joined_column\",\"Label\"]]","metadata":{"id":"fUMcCAyliInd","execution":{"iopub.status.busy":"2024-02-07T14:52:49.584733Z","iopub.execute_input":"2024-02-07T14:52:49.585277Z","iopub.status.idle":"2024-02-07T14:52:49.601526Z","shell.execute_reply.started":"2024-02-07T14:52:49.585246Z","shell.execute_reply":"2024-02-07T14:52:49.600453Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"dataset_3[\"Label\"].value_counts()","metadata":{"id":"7-nz3yltiInh","outputId":"e4f6a1ed-37c4-43ef-8b8a-3f774b7d6856","execution":{"iopub.status.busy":"2024-02-07T14:52:49.602919Z","iopub.execute_input":"2024-02-07T14:52:49.603271Z","iopub.status.idle":"2024-02-07T14:52:49.620896Z","shell.execute_reply.started":"2024-02-07T14:52:49.603241Z","shell.execute_reply":"2024-02-07T14:52:49.619570Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"Label\nAnalysis                 2777\nManagement               1453\nStrategy                 1329\nScience and Tech         1059\nFactual                   860\nEthics and Regulation     526\nTaxonomy                  502\nScience and tech           64\nEthics and regulation      36\nDefinition                  1\nScience and                 1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dataset_3 = dataset_3.replace(['Taxonomy'], 'taxonomy')\ndataset_3 = dataset_3.replace([ 'Science and Tech', 'Science and Tech ','Science and tech'], 'science and tech')\ndataset_3 = dataset_3.replace(['Strategy'], 'strategy')\ndataset_3 = dataset_3.replace(['Management'], 'management')\ndataset_3 = dataset_3.replace(['Analysis'], 'analysis')\ndataset_3 = dataset_3.replace(['Factual'], 'factual')\ndataset_3 = dataset_3.replace(['Ethics and regulation','Ethics and Regulation'], 'ethics and regulation')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.622558Z","iopub.execute_input":"2024-02-07T14:52:49.622956Z","iopub.status.idle":"2024-02-07T14:52:49.671955Z","shell.execute_reply.started":"2024-02-07T14:52:49.622923Z","shell.execute_reply":"2024-02-07T14:52:49.670505Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"dataset_3[\"Label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.673777Z","iopub.execute_input":"2024-02-07T14:52:49.674159Z","iopub.status.idle":"2024-02-07T14:52:49.685023Z","shell.execute_reply.started":"2024-02-07T14:52:49.674125Z","shell.execute_reply":"2024-02-07T14:52:49.683757Z"},"trusted":true},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"Label\nanalysis                 2777\nmanagement               1453\nstrategy                 1329\nscience and tech         1123\nfactual                   860\nethics and regulation     562\ntaxonomy                  502\nDefinition                  1\nScience and                 1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dataset_4 = dataset_3[(dataset_3[\"Label\"] == 'strategy') |\n                      (dataset_3[\"Label\"] == 'science and tech') |\n                      (dataset_3[\"Label\"] == 'analysis') |\n                      (dataset_3[\"Label\"] == 'factual') |\n                      (dataset_3[\"Label\"] == 'taxonomy') |\n                      (dataset_3[\"Label\"] == 'management') |\n                    (dataset_3[\"Label\"] == 'ethics and regulation')]","metadata":{"id":"rbJK9UIciInj","execution":{"iopub.status.busy":"2024-02-07T14:52:49.690377Z","iopub.execute_input":"2024-02-07T14:52:49.691069Z","iopub.status.idle":"2024-02-07T14:52:49.711880Z","shell.execute_reply.started":"2024-02-07T14:52:49.691026Z","shell.execute_reply":"2024-02-07T14:52:49.710838Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"dataset_4[\"Label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.713450Z","iopub.execute_input":"2024-02-07T14:52:49.713951Z","iopub.status.idle":"2024-02-07T14:52:49.724621Z","shell.execute_reply.started":"2024-02-07T14:52:49.713908Z","shell.execute_reply":"2024-02-07T14:52:49.723699Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"Label\nanalysis                 2777\nmanagement               1453\nstrategy                 1329\nscience and tech         1123\nfactual                   860\nethics and regulation     562\ntaxonomy                  502\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dataset_4","metadata":{"execution":{"iopub.status.busy":"2024-02-07T14:52:49.725934Z","iopub.execute_input":"2024-02-07T14:52:49.727076Z","iopub.status.idle":"2024-02-07T14:52:49.744102Z","shell.execute_reply.started":"2024-02-07T14:52:49.727034Z","shell.execute_reply":"2024-02-07T14:52:49.742926Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"                                          joined_column       Label\n0     What is the main strategy of Smart Cities like...    strategy\n1     What are the main components of Barcelona's Sm...    strategy\n2     Why are governments investing in advanced infr...  management\n3     How are cities transforming to enhance their i...    strategy\n4     How has Barcelona transformed itself into a Sm...  management\n...                                                 ...         ...\n8603  How many construction sites affected the stree...    analysis\n8604  How many trolleybuses need to be purchased by ...     factual\n8605  How many fixed commuters have been eliminated ...     factual\n8606  How many parking spaces were available in city...    analysis\n8607  How long do the works at the construction site...    analysis\n\n[8606 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>joined_column</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the main strategy of Smart Cities like...</td>\n      <td>strategy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are the main components of Barcelona's Sm...</td>\n      <td>strategy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why are governments investing in advanced infr...</td>\n      <td>management</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How are cities transforming to enhance their i...</td>\n      <td>strategy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How has Barcelona transformed itself into a Sm...</td>\n      <td>management</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8603</th>\n      <td>How many construction sites affected the stree...</td>\n      <td>analysis</td>\n    </tr>\n    <tr>\n      <th>8604</th>\n      <td>How many trolleybuses need to be purchased by ...</td>\n      <td>factual</td>\n    </tr>\n    <tr>\n      <th>8605</th>\n      <td>How many fixed commuters have been eliminated ...</td>\n      <td>factual</td>\n    </tr>\n    <tr>\n      <th>8606</th>\n      <td>How many parking spaces were available in city...</td>\n      <td>analysis</td>\n    </tr>\n    <tr>\n      <th>8607</th>\n      <td>How long do the works at the construction site...</td>\n      <td>analysis</td>\n    </tr>\n  </tbody>\n</table>\n<p>8606 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nnltk.download('all')","metadata":{"id":"_k4xqVUPiInm","execution":{"iopub.status.busy":"2024-02-07T14:52:49.745791Z","iopub.execute_input":"2024-02-07T14:52:49.746801Z","iopub.status.idle":"2024-02-07T14:52:52.634801Z","shell.execute_reply.started":"2024-02-07T14:52:49.746756Z","shell.execute_reply":"2024-02-07T14:52:52.633529Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n[nltk_data]    |       up-to-date!\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package bcp47 to /usr/share/nltk_data...\n[nltk_data]    |   Package bcp47 is already up-to-date!\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comparative_sentences is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Package dolch is already up-to-date!\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package extended_omw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package extended_omw is already up-to-date!\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package framenet_v15 is already up-to-date!\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package framenet_v17 is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package omw-1.4 is already up-to-date!\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n[nltk_data]    |   Package pe08 is already up-to-date!\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package perluniprops is already up-to-date!\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet3 is already up-to-date!\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wmt15_eval is already up-to-date!\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet2021 is already up-to-date!\n[nltk_data]    | Downloading package wordnet2022 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet2022 is already up-to-date!\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet31 is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n","output_type":"stream"},{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import re\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.tokenize import word_tokenize\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer() # lemma\n\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\n\ncorpus = []\n\ntext = list(dataset_4['joined_column']) # for cleaning purposes\n\nnumwords = {\n    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n    \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13, \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16,\n    \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19,\n    \"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50, \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90,\n    \"hundred\": 100, \"thousand\": 1000, \"million\": 1000000, \"billion\": 1000000000, \"trillion\": 1000000000000,\n}\n\nfor i in range(len(text)):\n\n    r = re.sub('[^a-zA-Z]', ' ', text[i])\n    r = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text[i])\n    r = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text[i])\n    \n    r = re.sub(r\"n\\'t\", \" not\", r)\n    r = re.sub(r\"\\'re\", \" are\", r)\n    r = re.sub(r\"\\'s\", \" is\", r)\n    r = re.sub(r\"\\'d\", \" would\", r)\n    r = re.sub(r\"\\'ll\", \" will\", r)\n    r = re.sub(r\"\\'t\", \" not\", r)\n    r = re.sub(r\"\\'ve\", \" have\", r)\n    r = re.sub(r\"\\'m\", \" am\", r)\n\n    r = re.sub(r\"n\\’t\", \" not\", r)\n    r = re.sub(r\"\\’re\", \" are\", r)\n    r = re.sub(r\"\\’s\", \" is\", r)\n    r = re.sub(r\"\\’d\", \" would\", r)\n    r = re.sub(r\"\\’ll\", \" will\", r)\n    r = re.sub(r\"\\’t\", \" not\", r)\n    r = re.sub(r\"\\’ve\", \" have\", r)\n    r = re.sub(r\"\\’m\", \" am\", r)\n    \n    # Convert text to integer\n    textnum = r.replace('-', ' ')\n    words = textnum.split()\n    result = 0\n    current = 0\n\n    for word in words:\n        if word in numwords:\n            num = numwords[word]\n            if num == 100:\n                current *= num\n            else:\n                current += num\n        elif word == 'and':\n            continue\n        else:\n            if current != 0:\n                result += current\n                current = 0\n\n    r = r.lower()\n\n    r = r.split()\n\n    r = [word for word in r if word not in stopwords.words('english')]\n\n    doc = nlp(text[i])\n    \n    r = [token.lemma_ for token in doc]\n\n    r = ' '.join(r)\n\n    corpus.append(r)\n","metadata":{"id":"Ky0PwF4JiInn","outputId":"107a2a90-45fd-44a9-b64f-e78a82647d69","execution":{"iopub.status.busy":"2024-02-07T14:52:52.636792Z","iopub.execute_input":"2024-02-07T14:52:52.637262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install spacy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_4['joined_column'] = corpus\n\ndataset_4.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_4.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report,make_scorer\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Preprocessing steps\nX = dataset_4['joined_column']\ny = dataset_4['Label']\n\n# Encode the target variable\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Create a TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=1000)\nX_tfidf = tfidf_vectorizer.fit_transform(X)\n\n","metadata":{"id":"X_RU6VOViInp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction import DictVectorizer\nfrom scipy.sparse import hstack \nfrom gensim.models import Word2Vec\nfrom scipy.sparse import csr_matrix\n\n\nimport torch\nX_feature_vectors = []  # Store the feature vectors for X\n\nfor text in X:\n    #tokens = text.split() \n    tokens = word_tokenize(text)\n    feature_dict = {}  # Local feature vector in dict form\n    for w in tokens:\n        if w in feature_dict:\n            feature_dict[w] = feature_dict[w] + 1  # Add weight based on occurrence\n        else:\n            feature_dict[w] = 1  # Assigning 1 to word weight\n    X_feature_vectors.append(feature_dict)\n\n# Convert X_feature_vectors into a sparse matrix\n\ndict_vectorizer = DictVectorizer(sparse=True)\nX_custom_features = dict_vectorizer.fit_transform(X_feature_vectors)\n\n# Load BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Initialize a list to store BERT embeddings\nX_bert_embeddings = []\n\nfor text in X:\n    # Tokenize the text using BERT tokenizer\n    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n    \n    # Pass the tokens through the BERT model to obtain embeddings\n    with torch.no_grad():\n        outputs = model(**tokens)\n    \n    # Extract the embeddings for the [CLS] token (you can use other tokens as well)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n    \n    X_bert_embeddings.append(embeddings)\n    \n# Convert X_bert_embeddings into a 2-D NumPy array\nX_bert_embeddings_array = np.array(X_bert_embeddings)\n\n# Create a sparse matrix from the NumPy array\nX_bert_embeddings_sparse = csr_matrix(X_bert_embeddings_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim.downloader as api\n\n# GloVe embeddings\nglove_model = api.load(\"glove-wiki-gigaword-100\")\nglove_size = 100  # You can change this based on the downloaded GloVe model\n\nX_glove_embeddings = []\n\nfor text in X:\n    tokens = word_tokenize(text)\n    embeddings = [glove_model[token] for token in tokens if token in glove_model]\n    if embeddings:\n        avg_embedding = sum(embeddings) / len(embeddings)\n        X_glove_embeddings.append(avg_embedding)\n    else:\n        X_glove_embeddings.append([0.0] * glove_size)\n\nX_glove_embeddings = np.array(X_glove_embeddings)\nX_glove_embeddings_sparse = csr_matrix(X_glove_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# Bag-of-Words (BoW) features\ncount_vectorizer = CountVectorizer(max_features=1000)\nX_bow = count_vectorizer.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize your text using NLTK\nX_tokenized = [word_tokenize(text) for text in X]\n\n# Train a Word2Vec model on your tokenized text\nw2v_model = Word2Vec(sentences=X_tokenized, vector_size=100, window=5, min_count=1, sg=0)\n\n# Initialize a list to store Word2Vec embeddings\nX_w2v_embeddings = []\n\nfor tokens in X_tokenized:\n    # Calculate the average embedding for the tokens in each text\n    embeddings = [w2v_model.wv[token] for token in tokens if token in w2v_model.wv]\n    if embeddings:\n        avg_embedding = sum(embeddings) / len(embeddings)\n        X_w2v_embeddings.append(avg_embedding)\n    else:\n        # If no tokens are in the Word2Vec vocabulary, add a placeholder vector\n        X_w2v_embeddings.append([0.0] * w2v_model.vector_size)\n\n# Convert X_bert_embeddings into a 2-D NumPy array\nX_w2v_embeddings = np.array(X_w2v_embeddings)\n\n# Convert X_w2v_embeddings into a sparse matrix\nX_w2v_embeddings = csr_matrix(X_w2v_embeddings)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nCombine TF-IDF features, custom features, and Word2Vec embeddings into a single sparse matrix\nX_combined = hstack([X_tfidf, X_custom_features, X_w2v_embeddings, X_bert_embeddings_sparse,X_bow,X_glove_embeddings_sparse])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" X_combined.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_4['QAL'] =dataset_4['joined_column'] + ' ' + dataset_4['Label']\ndataset_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/topics-for-modelling/Smart cities macro topic topics sub-topics.xlsx')\nsubs = df['Sub- topic '].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We define a number of topics that we know are in the documents\ntopic_list = subs\ntopic_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    #text = text.lower()\n\n    # Remove numbers, symbols, and punctuation (except for the case where 2 follows CO)\n    prep_text = re.sub(r'[\\d' + re.escape(string.punctuation) + '](?<![cC][oO]2)', '', text)\n\n    # Tokenize the text\n    tokens = word_tokenize(text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n\n    # Join the tokens back into a single string\n    preprocessed_text = ' '.join(tokens)\n\n    return preprocessed_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\nfrom nltk.tokenize import word_tokenize\ndataset_4['QAL_prep'] = dataset_4['QAL'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bertopic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keybert\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bertopic import BERTopic\n\nfrom bertopic.representation import KeyBERTInspired\n\n# Initialize and train the BERTopic model with fine-tuned hyperparameters\ntopic_model = BERTopic(\n    embedding_model=\"thenlper/gte-small\",\n    min_topic_size=10,  # Adjusted min_topic_size\n    zeroshot_topic_list=topic_list,\n    zeroshot_min_similarity=0.78,\n    representation_model=KeyBERTInspired()\n)\ntopics, probs = topic_model.fit_transform(dataset_4['QAL_prep'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.get_topic_info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.3 ,random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\n\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Define a list of classifiers\nclassifiers = [\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    SVC(),\n    KNeighborsClassifier(),\n    LogisticRegression()\n]\n\n# Train and evaluate each classifier\nfor classifier in classifiers:\n    # Train the classifier\n    classifier.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = classifier.predict(X_test)\n\n    # Evaluate performance\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    # Display results\n    print(f\"\\n{classifier.__class__.__name__} Results:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Classification Report:\\n{report}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.utils.class_weight import compute_class_weight\n\nsvc = SVC(probability=True)\n\nxgboost = XGBClassifier(\n    n_estimators=300,\n    max_depth=3,\n    learning_rate=0.1,\n    min_child_weight=2,\n    subsample=0.9,\n    colsample_bytree=0.8,\n    alpha=1,\n    gamma=0,\n    reg_lambda=2  # 'lambda' is a reserved keyword, use 'reg_lambda' instead\n)\n","metadata":{"id":"lXo86HMSz0lT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Identify minority classes\nminority_classes = [\"taxonomy\", \"ethics and regulation\"]\n\n# Filter instances belonging to minority classes\nminority_indices = np.isin(label_encoder.classes_, minority_classes)\nminority_class_labels = label_encoder.transform(minority_classes)\nX_train_minority = X_train[np.isin(y_train, minority_class_labels)]\n\n# Separate majority and minority instances\nX_majority = X_train[~np.isin(y_train, minority_class_labels)]\ny_majority = y_train[~np.isin(y_train, minority_class_labels)]\n\n# Print shapes before SMOTE transformation\nprint(\"Before SMOTE transformation:\")\nprint(\"Majority class shape:\", X_majority.shape)\nprint(\"Minority class shape:\", X_minority_resampled.shape)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resample only the minority class instances using SMOTE\ndesired_minority_ratio = 0.5  # Adjust this ratio based on your preference\n# Convert sparse matrices to dense arrays\n#X_majority_dense = X_majority.toarray()\n\n# Calculate the length of the minority class\nminority_length = int(desired_minority_ratio * X_majority.shape[0])\n\nsmote = SMOTE(sampling_strategy={1: minority_length, 6: minority_length}, random_state=42)\n\n\nX_minority_resampled, y_minority_resampled = smote.fit_resample(X_train_minority, y_train[np.isin(y_train, minority_class_labels)])\n# Combine the resampled minority instances with the original majority instances\nX_train_resampled = np.concatenate([X_majority.toarray(), X_minority_resampled.toarray()])\ny_train_resampled = np.hstack([y_majority, y_minority_resampled])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Print shapes after SMOTE transformation\nprint(\"\\nAfter SMOTE transformation:\")\nprint(\"Majority class shape:\", X_majority.shape)\nprint(\"Minority class shape:\", X_minority_resampled.shape)\nprint(\"Class Distribution in y_train_resampled:\", np.bincount(y_train_resampled))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\nprint(\"Class Weights:\", class_weights)\nprint(\"Unique Classes in y_train_resampled:\", np.unique(y_train_resampled))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update logistic regression model with class weights\nlogistic_regression = LogisticRegression(\n    class_weight= dict(zip(np.unique(y_train_resampled), class_weights)),\n    warm_start=True,\n    C=1,\n    fit_intercept=False,\n    penalty='l2',\n    max_iter=50,\n    multi_class='multinomial',\n    solver='saga'\n)\n\n# Create an ensemble model using VotingClassifier\nensemble_model = VotingClassifier(\n    estimators=[('logistic_regression', logistic_regression), ('xgboost', xgboost), ('svc', svc) ],\n    voting='soft'  # Use soft voting for probability-based predictions\n)\n\n# Fit the ensemble model to the resampled training data\nensemble_model.fit(X_train_resampled, y_train_resampled)\nX_test_dense = X_test.toarray()\n\n# Make predictions on the test set using the ensemble model\ny_pred = ensemble_model.predict(X_test_dense)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\nclassification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0)\n\nmetrics = {\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1': f1,\n    'Classification Report': classification_rep\n}\n\nprint(f'Accuracy: {metrics[\"Accuracy\"]:.4f}')\nprint(f'Precision: {metrics[\"Precision\"]:.4f}')\nprint(f'Recall: {metrics[\"Recall\"]:.4f}')\nprint(f'F1-score: {metrics[\"F1\"]:.4f}')\nprint(f'Classification Report:\\n{metrics[\"Classification Report\"]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Define your labels as a list of strings\nlabels = ['analysis', 'science and tech', 'factual', 'taxonomy', 'strategy','ethics and regulation', 'management']\n\n# Assuming y_test contains the true labels for the test set and y_pred contains predicted labels\n# Define a mapping from numerical labels to string labels\nlabel_mapping = {i: label for i, label in enumerate(labels)}\n\n# Use list comprehension to convert numerical labels to string labels\ny_test_strings = [label_mapping[label] for label in y_test]\ny_pred_strings = [label_mapping[label] for label in y_pred]\n\nconfusion = confusion_matrix(y_test_strings, y_pred_strings)\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion, display_labels=labels)\n\n# Rotate the x-axis labels for better alignment\ndisp.plot(cmap='viridis', xticks_rotation='vertical')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_1c = pd.read_csv('test.csv', encoding='ISO-8859-1',on_bad_lines='skip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_t2=dataset_t1.dropna() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_t2['joined_column'] = dataset_t2['Question'] + ' ' + dataset_t2['Answer']\n#dataset_t3 =dataset_t2[[\"joined_column\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_t3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_t3.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n\n# from nltk.corpus import stopwords\n\n# from nltk.tokenize import word_tokenize\n\n# from nltk.stem import WordNetLemmatizer\n\n# lemmatizer = WordNetLemmatizer() # lemma\n\n# import spacy\n\n# nlp = spacy.load('en_core_web_sm')\n\n# corpus = []\n\n# text = list(dataset_t3['joined_column']) # for cleaning purposes\n\n# numwords = {\n#     \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n#     \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13, \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16,\n#     \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19,\n#     \"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50, \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90,\n#     \"hundred\": 100, \"thousand\": 1000, \"million\": 1000000, \"billion\": 1000000000, \"trillion\": 1000000000000,\n# }\n\n# for i in range(len(text)):\n\n#     r = re.sub('[^a-zA-Z]', ' ', text[i])\n#     r = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text[i])\n#     r = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text[i])\n    \n#     r = re.sub(r\"n\\'t\", \" not\", r)\n#     r = re.sub(r\"\\'re\", \" are\", r)\n#     r = re.sub(r\"\\'s\", \" is\", r)\n#     r = re.sub(r\"\\'d\", \" would\", r)\n#     r = re.sub(r\"\\'ll\", \" will\", r)\n#     r = re.sub(r\"\\'t\", \" not\", r)\n#     r = re.sub(r\"\\'ve\", \" have\", r)\n#     r = re.sub(r\"\\'m\", \" am\", r)\n\n#     r = re.sub(r\"n\\’t\", \" not\", r)\n#     r = re.sub(r\"\\’re\", \" are\", r)\n#     r = re.sub(r\"\\’s\", \" is\", r)\n#     r = re.sub(r\"\\’d\", \" would\", r)\n#     r = re.sub(r\"\\’ll\", \" will\", r)\n#     r = re.sub(r\"\\’t\", \" not\", r)\n#     r = re.sub(r\"\\’ve\", \" have\", r)\n#     r = re.sub(r\"\\’m\", \" am\", r)\n    \n#     # Convert text to integer\n#     textnum = r.replace('-', ' ')\n#     words = textnum.split()\n#     result = 0\n#     current = 0\n\n#     for word in words:\n#         if word in numwords:\n#             num = numwords[word]\n#             if num == 100:\n#                 current *= num\n#             else:\n#                 current += num\n#         elif word == 'and':\n#             continue\n#         else:\n#             if current != 0:\n#                 result += current\n#                 current = 0\n\n#     r = r.lower()\n\n#     r = r.split()\n\n#     r = [word for word in r if word not in stopwords.words('english')]\n\n#     doc = nlp(text[i])\n    \n#     r = [token.lemma_ for token in doc]\n\n#     r = ' '.join(r)\n\n#     corpus.append(r)\n\n# dataset_t3['joined_column'] = corpus\n\n# dataset_t3.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Preprocessing steps\n# X = dataset_t3['joined_column']\n\n# X_feature_vectors = []  # Store the feature vectors for X\n\n# for text in X:\n#     #tokens = text.split() \n#     tokens = word_tokenize(text)\n#     feature_dict = {}  # Local feature vector in dict form\n#     for w in tokens:\n#         if w in feature_dict:\n#             feature_dict[w] = feature_dict[w] + 1  # Add weight based on occurrence\n#         else:\n#             feature_dict[w] = 1  # Assigning 1 to word weight\n#     X_feature_vectors.append(feature_dict)\n\n# # Convert X_feature_vectors into a sparse matrix\n\n# dict_vectorizer = DictVectorizer(sparse=True)\n# X_custom_features = dict_vectorizer.fit_transform(X_feature_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create a TF-IDF vectorizer\n# tfidf_vectorizer = TfidfVectorizer(max_features=7024)\n# X_tfidf = tfidf_vectorizer.fit_transform(X)\n# X_combined = hstack([X_tfidf, X_custom_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_features = 7024\n\n# # Select a subset of features from X_tfidf\n# X_tfidf_subset = X_tfidf[:, :max_features]\n\n# # Select a subset of features from X_custom_features\n# X_custom_features_subset = X_custom_features[:, :max_features]\n\n# # Horizontally stack the two subsets\n# X_combined = np.hstack([X_tfidf_subset, X_custom_features_subset])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred = ensemble_model.predict(X_combined)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datasetl = dataset_t2['Label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Calculate evaluation metrics\n# accuracy = accuracy_score(datasetl , y_pred)\n# precision = precision_score(datasetl , y_pred, average='weighted', zero_division=0)\n# recall = recall_score(datasetl , y_pred, average='weighted')\n# f1 = f1_score(datasetl , y_pred, average='weighted', zero_division=0)\n# classification_rep = classification_report(datasetl , y_pred, target_names=label_encoder.classes_, zero_division=0)\n\n# metrics = {\n#     'Accuracy': accuracy,\n#     'Precision': precision,\n#     'Recall': recall,\n#     'F1': f1,\n#     'Classification Report': classification_rep\n# }\n\n# print(f'Accuracy: {metrics[\"Accuracy\"]:.4f}')\n# print(f'Precision: {metrics[\"Precision\"]:.4f}')\n# print(f'Recall: {metrics[\"Recall\"]:.4f}')\n# print(f'F1-score: {metrics[\"F1\"]:.4f}')\n# print(f'Classification Report:\\n{metrics[\"Classification Report\"]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define your labels as a list of strings\n# labels = ['analysis', 'science and tech', 'factual', 'taxonomy', 'strategy', 'ethics and regulation', 'management']\n\n# # Assuming y_test contains the true labels for the test set and y_pred contains predicted labels\n# # Define a mapping from numerical labels to string labels\n# label_mapping = {i: label for i, label in enumerate(labels)}\n\n# # Use list comprehension to convert numerical labels to string labels\n# y_pred_strings = [label_mapping[label] for label in y_pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define your labels as a list of strings\n# labels = ['analysis', 'science and tech', 'factual', 'taxonomy', 'strategy', 'ethics and regulation', 'management']\n\n# # Assuming y_test contains the true labels for the test set and y_pred contains predicted labels\n# # Define a mapping from numerical labels to string labels\n# label_mapping = {i: label for i, label in enumerate(datasetl)\n#                 }# Use list comprehension to convert numerical labels to string labels\n# y_chatgpt_strings = [label_mapping[label] for label in y_pred]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Zip the two lists together\n# zipped_strings = list(zip(y_pred_strings, y_chatgpt_strings))\n\n# # Display the pairs side by side\n# for pred, chatgpt in zipped_strings:\n#     print(f'{pred}\\t{chatgpt}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}